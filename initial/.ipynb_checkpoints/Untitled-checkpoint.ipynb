{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fn_linear import fn_linear\n",
    "from fn_flatten import fn_flatten\n",
    "from fn_relu import fn_relu\n",
    "from fn_pool import fn_pool\n",
    "from linear3 import fn_conv\n",
    "from linear2 import fn_softmax\n",
    "from linear4 import loss_crossentropy \n",
    "from loss_euclidean import loss_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cal import calc_gradient\n",
    "from inf import inference\n",
    "from init_layers import init_layers\n",
    "from train import train\n",
    "from upd import update_weights \n",
    "# ate_weights_SGD_momenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1, 60000)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path += ['layers']\n",
    "# from loss_euclidean import loss_euclidean\n",
    "\n",
    "from load_MNIST_images import load_MNIST_images\n",
    "from load_MNIST_labels import load_MNIST_labels\n",
    "\n",
    "# Load training data\n",
    "train_data = load_MNIST_images('train-images.idx3-ubyte')\n",
    "print (train_data.shape)\n",
    "train_label = load_MNIST_labels('train-labels.idx1-ubyte')\n",
    "# Load testing data\n",
    "test_data = load_MNIST_images('t10k-images.idx3-ubyte')\n",
    "test_label = load_MNIST_labels('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_model import init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 1)\n",
      "Hyper dimension 1152 500\n",
      "(500, 1)\n",
      "Hyper dimension 500 10\n",
      "(1152, 5)\n",
      "Hyper dimension 1152 500\n",
      "(500, 5)\n",
      "Hyper dimension 500 10\n",
      "Input size:\n",
      "[28, 28, 1]\n",
      "Layer 0 output size: \n",
      "(26, 26, 6)\n",
      "Layer 1 output size: \n",
      "(13, 13, 6)\n",
      "Layer 2 output size: \n",
      "(12, 12, 32)\n",
      "Layer 3 output size: \n",
      "(6, 6, 32)\n",
      "Layer 4 output size: \n",
      "(6, 6, 32)\n",
      "Layer 5 output size: \n",
      "(1152,)\n",
      "Layer 6 output size: \n",
      "(500,)\n",
      "Layer 7 output size: \n",
      "(10,)\n",
      "Final output size:\n",
      "(10,)\n",
      "Provided output size (should match above):\n",
      "10\n",
      "(Batch dimension not included)\n"
     ]
    }
   ],
   "source": [
    "num_filters = 32\n",
    "trainshape = train_data.shape\n",
    "num_in = trainshape[0]*trainshape[1]*trainshape[2]*trainshape[3]\n",
    "\n",
    "l = [init_layers('conv', {'filter_size': 3,\n",
    "                          'filter_depth': 1,\n",
    "                          'num_filters': 6}),\n",
    "     init_layers('pool', {'filter_size': 2,\n",
    "                          'stride': 2}),\n",
    "     init_layers('conv', {'filter_size': 2,\n",
    "                          'filter_depth': 6,\n",
    "                          'num_filters': 32}),\n",
    "     init_layers('pool', {'filter_size': 2,\n",
    "                  'stride': 2}),\n",
    "     init_layers('relu', {}),\n",
    "     init_layers('flatten', {}),\n",
    "     init_layers('linear', {'num_in': 1152,\n",
    "                    'num_out': 500}),\n",
    "     init_layers('linear', {'num_in': 500,\n",
    "                            'num_out': 10}),\n",
    "     init_layers('softmax', {})]\n",
    "\n",
    "params = {\n",
    "#     \"learning_rate\": .01,\n",
    "#     \"weight_decay\": .0005,\n",
    "    \"learning_rate\": .05,\n",
    "    \"weight_decay\": .005,\n",
    "    \"batch_size\": 128,\n",
    "    \"save_file\": 'model.npz'\n",
    "}\n",
    "\n",
    "model = init_model(l, list(train_data[:,:,:,0].shape), 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.01\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  0 loss:  2.5354515098841697\n",
      "The accuracy is:  0.09375\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  5 loss:  2.3178929077321273\n",
      "The accuracy is:  0.1171875\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  10 loss:  2.304894763250016\n",
      "The accuracy is:  0.140625\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  15 loss:  2.307268564695994\n",
      "The accuracy is:  0.125\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  20 loss:  2.24725563466788\n",
      "The accuracy is:  0.1640625\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  25 loss:  2.2233925663599745\n",
      "The accuracy is:  0.21875\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  30 loss:  2.2389863879410545\n",
      "The accuracy is:  0.1171875\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  35 loss:  2.1970907413190672\n",
      "The accuracy is:  0.2421875\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "Epoch:  40 loss:  2.1641652197716326\n",
      "The accuracy is:  0.2734375\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(28, 28, 1, 128)\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(500, 128)\n",
      "Hyper dimension 500 10\n",
      "(1152, 128)\n",
      "Hyper dimension 1152 500\n"
     ]
    }
   ],
   "source": [
    "input = train_data\n",
    "numIters = 100\n",
    "label = train_label \n",
    "\n",
    "\n",
    "\n",
    "lr = params.get(\"learning_rate\", .05)\n",
    "# Weight decay\n",
    "wd = params.get(\"weight_decay\", .001)\n",
    "# Batch size\n",
    "batch_size = params.get(\"batch_size\", 128)\n",
    "# There is a good chance you will want to save your network model during/after\n",
    "# training. It is up to you where you save and how often you choose to back up\n",
    "# your model. By default the code saves the model in 'model.npz'.\n",
    "save_file = params.get(\"save_file\", 'model.npz')\n",
    "\n",
    "# update_params will be passed to your update_weights function.\n",
    "# This allows flexibility in case you want to implement extra features like momentum.\n",
    "update_params = {\"learning_rate\": lr,\n",
    "                 \"weight_decay\": wd }\n",
    "\n",
    "print(\"Learning Rate: \", lr)\n",
    "\n",
    "num_inputs = input.shape[-1]\n",
    "loss = np.zeros((numIters,))\n",
    "\n",
    "for i in range(numIters):\n",
    "    # TODO: One training iteration\n",
    "    # Steps:\n",
    "    #   (1) Select a subset of the input to use as a batch\n",
    "    #   (2) Run inference on the batch\n",
    "    #   (3) Calculate loss and determine accuracy\n",
    "    #   (4) Calculate gradients\n",
    "    #   (5) Update the weights of the model\n",
    "    # Optionally,\n",
    "    #   (1) Monitor the progress of training\n",
    "    #   (2) Save your learnt model, using ``np.savez(save_file, **model)``\n",
    "    batch_num = np.random.randint(num_inputs, size=batch_size)\n",
    "    sample_input = input[:,:,:,batch_num]\n",
    "    print(sample_input.shape)\n",
    "\n",
    "    sample_label = label[batch_num]\n",
    "    sample_output, sample_activations = inference(model, sample_input)\n",
    "    loss[i], dv_gradient = loss_crossentropy(sample_output, sample_label, {}, True)\n",
    "\n",
    "    if (i % 5 == 0):\n",
    "        print(\"Epoch: \", i, \"loss: \", loss[i])\n",
    "        count = 0.0\n",
    "        for j in range (batch_size):\n",
    "            if (np.argmax(sample_output[:,j]) == sample_label[j]):\n",
    "                count +=1.0\n",
    "        print(\"The accuracy is: \", count/batch_size)\n",
    "\n",
    "    sample_grads = calc_gradient(model, sample_input, sample_activations, dv_gradient)\n",
    "    model = update_weights (model, sample_grads, update_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0.0\n",
    "for j in range (batch_size):\n",
    "    if (np.argmax(sample_output[:,0]) == sample_label[j]):\n",
    "        count +=1.0\n",
    "print(\"The accuracy is: \", count/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
